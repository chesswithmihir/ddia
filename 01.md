# Chapter 1 Reliable, Scalable, and Maintainable Systems

## Introduction

> The Internet was done so well that most people think of it as a natural resource like the
Pacific Ocean, rather than something that was man-made. When was the last time a tech‐
nology with a scale like that was so error-free? <br>
—Alan Kay, in interview with Dr Dobb’s Journal (2012)

Many applications today are data-intensive (contrasted with CPU intensive because raw CPU power is rarely a limiting factor for these applications. Bigger problems are usually in the amount, complexity, and speed of data.

Data intensive apps have building blocks that provide for its functionality
- Store data so another app can find the data (DB)
- remember the result of an expensive operation (cache)
- query data via searching of keywords or filters (search indexes)
- send messages to another process to be handled asynchronously (stream processing)
- periodicaly crunch a large amount of accumulated data (batch processing)

Often there is no need to shift away from this abstraction since it's become perfected over time.

However, it's not enough to have these abstractions and call it a day. Many DB systems exist with different characteristics and support different requirements. When building an application, we need to figure out which tool and which approaches are most appropriate for the task at hand. And it can be hard to combine tools when you need to do something that a single tool cannot do alone.

Let's first go over what it means to have reliable, scalable, and maintainble data systems.

## Thinking about Data Systems

Why lump DB, queues, caches, etc all in Data Systems?

Many new tools for data storage and processing have emreged in recent years. They are optimized for a variety of different use cases, and they no longer neatly fit into traditional categories. For example, there are datastores that are also used as message queues (Redis), and there are message queues with database-like durability guarantees (Apache Kafka). The boundaries between the categories are becoming blurred. 

many apps now have demanding or wide-ranging reqs that a single tool can no longer meet all of its data processing and storage needs. Instead the work is broken down into tasks that can be perfomed efficinetly on a single tools, and those different tools are stitched together using application code.

For example, if you have an application-managed caching layer (using memcached or similar) or a full-text search server such as Elasticsearch or Solr) separate from your main db, it's normally the app code's responsibility to keep those cahces and indexes in sync with the main db. 

<img width="967" alt="Screenshot 2024-07-20 at 11 45 39 AM" src="https://github.com/user-attachments/assets/fcd52cbe-4e98-46d3-ae23-4a3206648d52">
<em>Figure 1-1. One possible architecture for a data system that combines several
components</em>

When you combine several tools in order to provide a service, the service's interface or application programming interface (API) usually hides those implementations details from clients. Now you have essentially created a new, special purpose data system from smaller, general-purpose components. Your composite data system may provide certain guarantees: eg, that the cache will be correctly invalidated or updated on writes so that outside clients see consistent results. You are now not only an app developer but also a data system designer.

If you are designing a data system or service, a lot of tricky questions arise. How do you ensure data is correct and complete? What if things fail internally? How do you provide good performance to clients? How do you scale with an increase in load? What does a good API for the service look like?

There are many factors that may influence the design of a data system, including the skills and experience of the people involved, legacy systems dependencies, the time scale for delivery, your organizations's tolerance of different kinds of risk, regulatory constraints, etc. Those factors depend very much on the situation.

In this book, we focus on three concerns that are important in most software systems:

Reliability
- The system should continue to work correctly (performing the correct function at the desired level of performance) even in the face of adversity (hardware or software faults, and even human error)

Scalability
- As the system grows (in data volume, traffic volume, or complexity) there should be reasonable ways of dealing iwth that growth.

Maintainability
- Over time, many different people will work on the system (engineering and operations, both maintaining current behaviour and adapting the system to new use cases), and tehy should all be able to work on it productively.

These words are often cast around without a clear understanding of what they mean. In the interest of thoughtful engineering, we will spend the rest of this chapter exploring ways of thinking about reliability, scalability, and maintainability. Then, in the following chapters, we will look at various techniques, architectures, and algorithms that are used in order to achieve those goals.

## Reliability

Everybody has an intuitive idea of what it means for something to be reliable or unreliable. For software, typical expectations include:

- the application performs the function that the user expected.
- It can tolerate the user making mistakes or using the software in unexpected ways
- It's performance is good enough for the required use case, under the expected load and data volume
- The system prevents any unauthorized access and abuse.

If all those things together mean "working correctly", then we can understand reliability as meaning roughly continuing to work correctly even when things go wrong.

The things that can go wrong are called faults, and systems that anticipate faults and systems that anticipate faults and can cope with them are called fault-tolerant and resilient. The former term is slightly misleading: it suggests that we could make a system tolerant of every possible kind of fault, which in reality is not feasible. If the entire planet Earth (and all servers on it) were swallowed by a black hole, tolerance of that fault would require web hosting in space -- good luck getting that budget item approved. So it makes sense to talk about certain types of faults that are more realistic.

fault != failure: A fault is usually defined as one component of the system deviating from its spec, whereas a failure is when the system as a whole stops providing the required service to the user. It is imposible to reduce the probability of a fuault to zero, therefore it is usually best to design fault-tolerance mechanisms that prevent faults from causing failures. Counterintuitively, in such fault-tolerant systems, it can make sense to increase the rate of faults by triggering them deliberately for example by randomly terminating individual processes without warning. Many critical bugs are actually due to poor error handling. By deliberatly inducing faults you ensure that the fault tolerance machinery is continually exercised and tested, which can increase your confidence that faults will be handled correctly when they occur naturally. The Netflix Chaos Monkey is an example of this approach.

...

> A not about Chaos Monkey from [Netflix Engineering Blog](https://netflixtechblog.com/the-netflix-simian-army-16e57fbab116)

John Ciancutti was a big engineer at Netflix (moved to FB, Google, Coursera now Amazon Music). He joined Netflix in 1999 as a Sr SWE). John shared improvements in cloud especially around reliability and availability. Cloud is all about redundance and fault-tolerance. Since no single component can guarantee 100% uptime (and even the most expensive hardware eventually fails), we have to design a cloud architecture where individual components can fail without affecting the availability of the entire system. In effect, we have to be stronger than our weakest link. We can use techniques like graceful degradation on dependency failures. 

> graceful degradation: <br>
(the ability of a computer, machine, electronic system or network to maintain limited functionality even when a large portion of it has been destroyed or rendered inoperative. The purpose of graceful degradation is to prevent catastrophic failure.) It is a level below fault-tolerant systems, which continue running at the same rate of speed. For example, a two-computer complex employing graceful degradation would be reduced to using one system if the other fails. With fault tolerance, a third computer would be standing by to take over in the event of failure. node-, rack-, datacenter-/availability-zone-, and even regionally-redundant deployments. But just designing a fault tolerant architecture is not enough. We have to constantly test our ability to actually survive these “once in a blue moon” failures.

Imagine getting a flat tire. Even if you have a spare tire in your trunk, do you know if it is inflated? Do you have the tools to change it? And, most importantly, do you remember how to do it right? One way to make sure you can deal with a flat tire on the freeway, in the rain, in the middle of the night is to poke a hole in your tire once a week in your driveway on a Sunday afternoon and go through the drill of replacing it. This is expensive and time-consuming in the real world, but can be (almost) free and automated in the cloud.

This is the same philosophy that went into making Chaos Monkey, a tool that randomly disables our production instances to make sure we can survive this common type of failure without any customer impact. The name comes from the idea of unleashing a wild monkey with a weapon in your data center (or cloud region) to randomly shoot down instances and chew through cables -- all the while we continue serving our customers without interruption. By running chaos monkey in the middle of a business day, in a carefully monitored environment with engineers standing by to address any problems, we can still learn the lessons about the weaknesses of our system and build automatic recovery mechanisms to deal with them. So next time an instance fails at 3am on a Sunday we won't even notice.


Netflix has created similar Simians to induce several types of failures and test the ability to survive them: Latency monkey (delays in RESTful client-server communication layer), conformity monkey (relaunch servers that don't adhere to best practices), Doctor Monkey (health checks on CPU load), Janitor Monkey (ensures that our cloud env is running free of clutter and waste, unused resources, maybe oom), security monkey (finds security violations such as improperly configured AWS security groups, SSL, DRM certificats), 10-18 monkey (short for localization-internalization, or l10n-i18n, config + runtime problems), chaos gorilla (outage of entire AWS availability zone). Simian army helps test resilience into Netflix. Blog by Yury Izrailevsky, Director of Cloud & Systems Infrastructure and Ariel Tseitlin, Director of Cloud Solutions.

...

Although we generally prefer tolerating faults over preventing faults there are cases where prevention is better than cure (because no cure exists) such as if sensitive data is leaked in a security vulnerability. Let's look at the faults that can be cured:



## Hardware Faults

Hard disk crash, RAM becomes faulty, the power grid has a blackout, somone unplugs the wrong network cable. happens all the time with large datacenters, especially with a lot of machines.

Hard disks are reported as having a MTTF (mean time to failure) of about 10 - 50 years. Thus on a storage cluster with 10k disks, one should fail per day.

Our first response is usally to add redundancy to hardware. Disks may be setup in a RAID configuration, servers may have dual power supplies and hot-swappable CPUs and datacenters may have batteries and diesel generators for backup power. When one component goes, the redundant component can take its place while the broken component is replaced. This approach cannot completely prevent hardware problems from causing failures but it is well understood and can often keep a machine running interrupted for years.

Until recently, redundance of hardware components was sufficient for most applications, since it makes total failure of a single machine fairly rare. As long as you can restore a backup onto a new machine fairly quickly, the downtime in case of failure is not catastrophic in most applications. However as data volumes and applications computing demands have increased more applications have begun using larger numbers of machines which proportionally increases the rate of hardware faults. Moreover, in some cloud platforms such as AWS, it is faily common for virtual machine instances to become unavailable without warning as the platforms are designed to priority flexibility and elasticity over single machine reliability. 

Hence there is a move towards systems that can tolerate the loss of entre machines, by using software fault tolerance techniques in preference or in addition to hardware redundancy. Rolling upgrades are essential here to build systems that can tolerate machines and patch one node at a time instead of having downtime of the entire system. Rolling Upgrades are discussed more in Chapter 4.

## Software Errors

Software errors are often correlated across multiple nodes. They tend to cause more system failures than uncorrelated hardware faults where if one machine dies, the other machines can pick up and are unaffected.

- a software bug that causes every instance of an app server to crash given a particular bad input. For example, consider the leap second on June 30, 2012, that caused many applications to hang simultaneously due to a bug in the linux kernel. When the leap second occurred, the Linux kernel mishandled the time adjustment, causing the kernel to enter a high CPU usage state, effectively hanging the system. This bug impacted numerous applications running on affected servers, leading to widespread service disruptions.

- a runaway process that uses up some shared resource - CPU time, memory, disk space or network bandwidth. A runaway process typically runs in an infinite loop or spawns new processes.

- A service that the system depends on that slows down, becomes unresponsive, or starts returning corrupted responses.

- Cascading failures, where a small fault in one component triggers a fault in another component, which in turn triggers further faults.

unusual set of circumstances can cause the above and active after being dormant for a long time. It is revealed that software is making an assumption of the environment and while that assumption is usually true, it eventuall can stop being true for some reason. Detect if can't prevent is pretty much the only solution here.

## Human Errors

Even when they have the best intentions, humans are known to be unreliable. Config errors by operators were the leading cause of outages whereas hardware faults played a role in only 10-25% of outages.

How do we make out system reliable in spite of unrelaible humans? The best system combines several approaches:

1. Design systems that minimize opportunity for error. For example, well designed abstractions, APIs and admin interfaces make it easy to do the right thing and discourage the wrong thing. However if the interface are too restrictive people will work around them, negating their benefit, so this is a tricky balance to get right.
2. Decouple the places where people make the most mistakes from the places where they can cause failures. In particular, provide fully featured non-prod sandbox enviornments where people can explore and experiment safely, using real data without affecting real users.
3. Test thoroughly at all levels, from unit tests to whole system integration tests and manual tests. Automated testing is widely used, well understood, and especially valuable for covering corner cases that rarely arise in normal operation.
4. Allow quick and easy recovery from human errors, to minimuze the impact in the case of a failure. For example, making it fast to roll back configuration changes, roll out new code gradually (so that any unexpected bugs affect only a small subset of users and provide tools to recompute data (in case it turns out that the old computation was incorrect).
5. Set up detailed and clear monitoring, such as performance metrics and error rates. In other engineering disciplines this is referred to as telemetry. Once a rocket has left the ground, the telemetry is essential for tracking what is happening, and for understanding failures. Monitoring can show us early warning signals and allow us to check whether any assumptions or constraints are being violated. When a problem occurs, metrics can be invaluable in diagnosing the issues.
6. Implement good management practices and training -- a complex and important aspect, and beyond the scope of this book.

## How important is reliability?

Bugs in apps cause lost productivity and legal risks (remember Shardul threads in wells fargo where customers saw each others data) and outages of ecommerce sites can have huge costs in terms of lost revenue and damage to reputation.

Even in noncritical apps we have a responsibility to our users. Consider a parent who stores all their pictures and videos of their children in your photo application. How would they feel if that database was suddenly corrupted? Would they know how to restore it from a backup?

There are situations in which we may choose to sacfrifice reliability in order to reduce a development cost (when developing a prototype product for an unproven market) or operational cost (for a service with a very narrow profit margin) but we should be very conscious of when we are cutting corners.

## Scalability

A system's ability to cope with increased load. This is not a one dimensional label that we can attach to a system: it is meaningless to say X is scalable or Y doesn't scale. Rather, discussing scalability means considering questions like if the system grows in a particular way, what are the options for coping with the growth? How can we add computing resources to handle the additional load?

## Describing Load



## Describing Performance


## Approaches for Coping with Load


## Maintainability


## Operability: Making life easy for operations


## Simplicity: Managing complexity


## Evolvability: Making change easy


## Summary

We saw fundamental ways of thinking about data intensive applications. Dive deep into technical detail in the rest of the book.

functional requirements (what it should do such as allowing data to be stored, retrieved, searched, and processed in various ways)

non-functional requirements (security, reliability, compliance, scalability, compatibility, maintainability). We discussed rel, scal, and main in detail, we will look at others in the next few chapters.

1. Reliability means making systems work correctly even when faults occur. Faults can be in hardware or software or humans and fault tolerance techniques can hide certain types of faults from the end user.

2. Scalability means having strategies for keeping performance good, even when load increases. In order to discuss scalability, we first need ways of describing load and performance quantitatively. We briefly looked at Twitter's home timelines as an example of describing load, and response time percentiles as a way of measureing perfromance. In a scalable system, you can add processing capacity in order to remain reliable under high load.

3. Maintainability has many facets but in essence it's about making life better for the engineering and operations teams who need to work with the system. Good abstractions can help reduce complexity and make the system easier to modify and adapt for new use cases. Good oeprability means having good visibility into the system's health and having effective ways of managing it.

There is no easy fix to have these 3 in place however certain patterns and techniques can help keep reappearing in different kinds of applications. In the next few chapters we will take a look at some examples of data systems and analyze how they work toward those goals. 

Later in the book, in part 3, we will look at patterns for systems that consist of several components working together such as the one in Figure 1-1 (one possible archtiecture for a data aystem that combines several components).
