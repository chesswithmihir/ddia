# Chapter 1 Reliable, Scalable, and Maintainable Systems

## Introduction

> The Internet was done so well that most people think of it as a natural resource like the
Pacific Ocean, rather than something that was man-made. When was the last time a tech‐
nology with a scale like that was so error-free? <br>
—Alan Kay, in interview with Dr Dobb’s Journal (2012)

Many applications today are data-intensive (contrasted with CPU intensive because raw CPU power is rarely a limiting factor for these applications. Bigger problems are usually in the amount, complexity, and speed of data.

Data intensive apps have building blocks that provide for its functionality
- Store data so another app can find the data (DB)
- remember the result of an expensive operation (cache)
- query data via searching of keywords or filters (search indexes)
- send messages to another process to be handled asynchronously (stream processing)
- periodicaly crunch a large amount of accumulated data (batch processing)

Often there is no need to shift away from this abstraction since it's become perfected over time.

However, it's not enough to have these abstractions and call it a day. Many DB systems exist with different characteristics and support different requirements. When building an application, we need to figure out which tool and which approaches are most appropriate for the task at hand. And it can be hard to combine tools when you need to do something that a single tool cannot do alone.

Let's first go over what it means to have reliable, scalable, and maintainble data systems.

## Thinking about Data Systems

Why lump DB, queues, caches, etc all in Data Systems?

Many new tools for data storage and processing have emreged in recent years. They are optimized for a variety of different use cases, and they no longer neatly fit into traditional categories. For example, there are datastores that are also used as message queues (Redis), and there are message queues with database-like durability guarantees (Apache Kafka). The boundaries between the categories are becoming blurred. 

many apps now have demanding or wide-ranging reqs that a single tool can no longer meet all of its data processing and storage needs. Instead the work is broken down into tasks that can be perfomed efficinetly on a single tools, and those different tools are stitched together using application code.

For example, if you have an application-managed caching layer (using memcached or similar) or a full-text search server such as Elasticsearch or Solr) separate from your main db, it's normally the app code's responsibility to keep those cahces and indexes in sync with the main db. 

<img width="967" alt="Screenshot 2024-07-20 at 11 45 39 AM" src="https://github.com/user-attachments/assets/fcd52cbe-4e98-46d3-ae23-4a3206648d52">
<em>Figure 1-1. One possible architecture for a data system that combines several
components</em>

When you combine several tools in order to provide a service, the service's interface or application programming interface (API) usually hides those implementations details from clients. Now you have essentially created a new, special purpose data system from smaller, general-purpose components. Your composite data system may provide certain guarantees: eg, that the cache will be correctly invalidated or updated on writes so that outside clients see consistent results. You are now not only an app developer but also a data system designer.

If you are designing a data system or service, a lot of tricky questions arise. How do you ensure data is correct and complete? What if things fail internally? How do you provide good performance to clients? How do you scale with an increase in load? What does a good API for the service look like?

There are many factors that may influence the design of a data system, including the skills and experience of the people involved, legacy systems dependencies, the time scale for delivery, your organizations's tolerance of different kinds of risk, regulatory constraints, etc. Those factors depend very much on the situation.

In this book, we focus on three concerns that are important in most software systems:

Reliability
- The system should continue to work correctly (performing the correct function at the desired level of performance) even in the face of adversity (hardware or software faults, and even human error)

Scalability
- As the system grows (in data volume, traffic volume, or complexity) there should be reasonable ways of dealing iwth that growth.

Maintainability
- Over time, many different people will work on the system (engineering and operations, both maintaining current behaviour and adapting the system to new use cases), and tehy should all be able to work on it productively.

These words are often cast around without a clear understanding of what they mean. In the interest of thoughtful engineering, we will spend the rest of this chapter exploring ways of thinking about reliability, scalability, and maintainability. Then, in the following chapters, we will look at various techniques, architectures, and algorithms that are used in order to achieve those goals.

## Reliability

Everybody has an intuitive idea of what it means for something to be reliable or unreliable. For software, typical expectations include:

- the application performs the function that the user expected.
- It can tolerate the user making mistakes or using the software in unexpected ways
- It's performance is good enough for the required use case, under the expected load and data volume
- The system prevents any unauthorized access and abuse.

If all those things together mean "working correctly", then we can understand reliability as meaning roughly continuing to work correctly even when things go wrong.

The things that can go wrong are called faults, and systems that anticipate faults and systems that anticipate faults and can cope with them are called fault-tolerant and resilient. The former term is slightly misleading: it suggests that we could make a system tolerant of every possible kind of fault, which in reality is not feasible. If the entire planet Earth (and all servers on it) were swallowed by a black hole, tolerance of that fault would require web hosting in space -- good luck getting that budget item approved. So it makes sense to talk about certain types of faults that are more realistic.

fault != failure: A fault is usually defined as one component of the system deviating from its spec, whereas a failure is when the system as a whole stops providing the required service to the user. It is imposible to reduce the probability of a fuault to zero, therefore it is usually best to design fault-tolerance mechanisms that prevent faults from causing failures. Counterintuitively, in such fault-tolerant systems, it can make sense to increase the rate of faults by triggering them deliberately for example by randomly terminating individual processes without warning. Many critical bugs are actually due to poor error handling. By deliberatly inducing faults you ensure that the fault tolerance machinery is continually exercised and tested, which can increase your confidence that faults will be handled correctly when they occur naturally. The Netflix Chaos Monkey is an example of this approach.

...
> A not about Chaos Monkey from [Netflix Engineering Blog](https://netflixtechblog.com/the-netflix-simian-army-16e57fbab116)


...
Although we generally prefer tolerating faults over preventing faults there are cases where prevention is better than cure (because no cure exists). This is the case. 


