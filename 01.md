# Chapter 1 Reliable, Scalable, and Maintainable Systems

## Introduction

> The Internet was done so well that most people think of it as a natural resource like the
Pacific Ocean, rather than something that was man-made. When was the last time a tech‐
nology with a scale like that was so error-free? <br>
—Alan Kay, in interview with Dr Dobb’s Journal (2012)

Many applications today are data-intensive (contrasted with CPU intensive because raw CPU power is rarely a limiting factor for these applications. Bigger problems are usually in the amount, complexity, and speed of data.

Data intensive apps have building blocks that provide for its functionality
- Store data so another app can find the data (DB)
- remember the result of an expensive operation (cache)
- query data via searching of keywords or filters (search indexes)
- send messages to another process to be handled asynchronously (stream processing)
- periodicaly crunch a large amount of accumulated data (batch processing)

Often there is no need to shift away from this abstraction since it's become perfected over time.

However, it's not enough to have these abstractions and call it a day. Many DB systems exist with different characteristics and support different requirements. When building an application, we need to figure out which tool and which approaches are most appropriate for the task at hand. And it can be hard to combine tools when you need to do something that a single tool cannot do alone.

Let's first go over what it means to have reliable, scalable, and maintainble data systems.

## Thinking about Data Systems

Why lump DB, queues, caches, etc all in Data Systems?

Many new tools for data storage and processing have emreged in recent years. They are optimized for a variety of different use cases, and they no longer neatly fit into traditional categories. For example, there are datastores that are also used as message queues (Redis), and there are message queues with database-like durability guarantees (Apache Kafka). The boundaries between the categories are becoming blurred. 

many apps now have demanding or wide-ranging reqs that a single tool can no longer meet all of its data processing and storage needs. Instead the work is broken down into tasks that can be perfomed efficinetly on a single tools, and those different tools are stitched together using application code.

For example, if you have an application-managed caching layer (using memcached or similar) or a full-text search server such as Elasticsearch or Solr) separate from your main db, it's normally the app code's responsibility to keep those cahces and indexes in sync with the main db. 

<img width="967" alt="Screenshot 2024-07-20 at 11 45 39 AM" src="https://github.com/user-attachments/assets/fcd52cbe-4e98-46d3-ae23-4a3206648d52">
<em>Figure 1-1. One possible architecture for a data system that combines several
components</em>

When you combine several tools in order to provide a service, the service's interface or application programming interface (API) usually hides those implementations details from clients. Now you have essentially created a new, special purpose data system from smaller, general-purpose components. Your composite data system may provide certain guarantees: eg, that the cache will be correctly invalidated or updated on writes so that outside clients see consistent results. You are now not only an app developer but also a data system designer.

If you are designing a data system or service, a lot of tricky questions arise. How do you ensure data is correct and complete? What if things fail internally? How do you provide good performance to clients? How do you scale with an increase in load? What does a good API for the service look like?

There are many factors that may influence the design of a data system, including the skills and experience of the people involved, legacy systems dependencies, the time scale for delivery, your organizations's tolerance of different kinds of risk, regulatory constraints, etc. Those factors depend very much on the situation.

In this book, we focus on three concerns that are important in most software systems:

Reliability
- The system should continue to work correctly (performing the correct function at the desired level of performance) even in the face of adversity (hardware or software faults, and even human error)

Scalability
- As the system grows (in data volume, traffic volume, or complexity) there should be reasonable ways of dealing iwth that growth.

Maintainability
- Over time, many different people will work on the system (engineering and operations, both maintaining current behaviour and adapting the system to new use cases), and tehy should all be able to work on it productively.

These words are often cast around without a clear understanding of what they mean. In the interest of thoughtful engineering, we will spend the rest of this chapter exploring ways of thinking about reliability, scalability, and maintainability. Then, in the following chapters, we will look at various techniques, architectures, and algorithms that are used in order to achieve those goals.

## Reliability

Everybody has an intuitive idea of what it means for something to be reliable or unreliable. For software, typical expectations include:

- the application performs the function that the user expected.
- It can tolerate the user making mistakes or using the software in unexpected ways
- It's performance is good enough for the required use case, under the expected load and data volume
- The system prevents any unauthorized access and abuse.

If all those things together mean "working correctly", then we can understand reliability as meaning roughly continuing to work correctly even when things go wrong.

The things that can go wrong are called faults, and systems that anticipate faults and systems that anticipate faults and can cope with them are called fault-tolerant and resilient. The former term is slightly misleading: it suggests that we could make a system tolerant of every possible kind of fault, which in reality is not feasible. If the entire planet Earth (and all servers on it) were swallowed by a black hole, tolerance of that fault would require web hosting in space -- good luck getting that budget item approved. So it makes sense to talk about certain types of faults that are more realistic.

fault != failure: A fault is usually defined as one component of the system deviating from its spec, whereas a failure is when the system as a whole stops providing the required service to the user. It is imposible to reduce the probability of a fuault to zero, therefore it is usually best to design fault-tolerance mechanisms that prevent faults from causing failures. Counterintuitively, in such fault-tolerant systems, it can make sense to increase the rate of faults by triggering them deliberately for example by randomly terminating individual processes without warning. Many critical bugs are actually due to poor error handling. By deliberatly inducing faults you ensure that the fault tolerance machinery is continually exercised and tested, which can increase your confidence that faults will be handled correctly when they occur naturally. The Netflix Chaos Monkey is an example of this approach.

...

> A not about Chaos Monkey from [Netflix Engineering Blog](https://netflixtechblog.com/the-netflix-simian-army-16e57fbab116)

John Ciancutti was a big engineer at Netflix (moved to FB, Google, Coursera now Amazon Music). He joined Netflix in 1999 as a Sr SWE). John shared improvements in cloud especially around reliability and availability. Cloud is all about redundance and fault-tolerance. Since no single component can guarantee 100% uptime (and even the most expensive hardware eventually fails), we have to design a cloud architecture where individual components can fail without affecting the availability of the entire system. In effect, we have to be stronger than our weakest link. We can use techniques like graceful degradation on dependency failures. 

> graceful degradation: <br>
(the ability of a computer, machine, electronic system or network to maintain limited functionality even when a large portion of it has been destroyed or rendered inoperative. The purpose of graceful degradation is to prevent catastrophic failure.) It is a level below fault-tolerant systems, which continue running at the same rate of speed. For example, a two-computer complex employing graceful degradation would be reduced to using one system if the other fails. With fault tolerance, a third computer would be standing by to take over in the event of failure. node-, rack-, datacenter-/availability-zone-, and even regionally-redundant deployments. But just designing a fault tolerant architecture is not enough. We have to constantly test our ability to actually survive these “once in a blue moon” failures.

Imagine getting a flat tire. Even if you have a spare tire in your trunk, do you know if it is inflated? Do you have the tools to change it? And, most importantly, do you remember how to do it right? One way to make sure you can deal with a flat tire on the freeway, in the rain, in the middle of the night is to poke a hole in your tire once a week in your driveway on a Sunday afternoon and go through the drill of replacing it. This is expensive and time-consuming in the real world, but can be (almost) free and automated in the cloud.

This is the same philosophy that went into making Chaos Monkey, a tool that randomly disables our production instances to make sure we can survive this common type of failure without any customer impact. The name comes from the idea of unleashing a wild monkey with a weapon in your data center (or cloud region) to randomly shoot down instances and chew through cables -- all the while we continue serving our customers without interruption. By running chaos monkey in the middle of a business day, in a carefully monitored environment with engineers standing by to address any problems, we can still learn the lessons about the weaknesses of our system and build automatic recovery mechanisms to deal with them. So next time an instance fails at 3am on a Sunday we won't even notice.


Netflix has created similar Simians to induce several types of failures and test the ability to survive them: Latency monkey (delays in RESTful client-server communication layer), conformity monkey (relaunch servers that don't adhere to best practices), Doctor Monkey (health checks on CPU load), Janitor Monkey (ensures that our cloud env is running free of clutter and waste, unused resources, maybe oom), security monkey (finds security violations such as improperly configured AWS security groups, SSL, DRM certificats), 10-18 monkey (short for localization-internalization, or l10n-i18n, config + runtime problems), chaos gorilla (outage of entire AWS availability zone). Simian army helps test resilience into Netflix. Blog by Yury Izrailevsky, Director of Cloud & Systems Infrastructure and Ariel Tseitlin, Director of Cloud Solutions.

...

Although we generally prefer tolerating faults over preventing faults there are cases where prevention is better than cure (because no cure exists) such as if sensitive data is leaked in a security vulnerability. Let's look at the faults that can be cured:



## Hardware Faults

Hard disk crash, RAM becomes faulty, the power grid has a blackout, somone unplugs the wrong network cable. happens all the time with large datacenters, especially with a lot of machines.

Hard disks are reported as having a MTTF (mean time to failure) of about 10 - 50 years. Thus on a storage cluster with 10k disks, one should fail per day.

Our first response is usally to add redundancy to hardware. Disks may be setup in a RAID configuration, servers may have dual power supplies and hot-swappable CPUs and datacenters may have batteries and diesel generators for backup power. When one component goes, the redundant component can take its place while the broken component is replaced. This approach cannot completely prevent hardware problems from causing failures but it is well understood and can often keep a machine running interrupted for years.

Until recently, redundance of hardware components was sufficient for most applications, since it makes total failure of a single machine fairly rare. As long as you can restore a backup onto a new machine fairly quickly, the downtime in case of failure is not catastrophic in most applications. However as data volumes and applications computing demands have increased more applications have begun using larger numbers of machines which proportionally increases the rate of hardware faults. Moreover, in some cloud platforms such as AWS, it is faily common for virtual machine instances to become unavailable without warning as the platforms are designed to priority flexibility and elasticity over single machine reliability. 

Hence there is a move towards systems that can tolerate the loss of entre machines, by using software fault tolerance techniques in preference or in addition to hardware redundancy. Rolling upgrades are essential here to build systems that can tolerate machines and patch one node at a time instead of having downtime of the entire system. Rolling Upgrades are discussed more in Chapter 4.

## Software Errors


## Human Errors


## How important is reliability?


## Scalability


## Describing Load


## Describing Performance


## Approaches for Coping with Load


## Maintainability


## Operability: Making life easy for operations


## Simplicity: Managing complexity


## Evolvability: Making change easy


## Summary
