# Chapter 11 Stream Processing

## Introduction

- Chapter 10 discusses batch processings
  - reading a set of files as input and producing a new set of output files (derived data)
-  Running the batch process again should yield the same derived dataset.
-  This can be used to create search indexes, recommendation systems, analytics, and more
- this assumes however, that the input is bounded (finite). Batch process knows when its finished reading input.
- It could be that the very last input record is the one with the lowest key so starting the output early is not an option when it comes to stream processing
- A lot of data is unbounded because it arrives gradually over time in the real world. Your users produced data yesterday and today and they will continue to produce more data tomorrow.
- Batch processors must thus divide the data into chunks of fixed duration. It is almost like discrete intervals must be taken for this to work.
- The problem with daily batch processes is that changes in the input are only reflected in the output a day later which is too slow for many impatient users.
- To reduce the delay we can minimize the interval from a day to a second.

> Stream Processings is the idea that we can process a second's worth of data at the end of every second or even continuously abandoning the fixed time slice entirely and simply processing every event as it happens like a "stream".

- `stdin` and `stdout` both involve streams because over time there is data that is being made available.
- There are also filesystem APIs such as Java's FileInputStream, TCP connections, delivering audio and video over the internet and so on.

> event streams are unbounded, incrememntally processed counterparts to the batch data we saw in chapter 10. First we will go into how streams are represnted, stored, and transmitted over a network. In "Databases and Streams" on page 451 we will investigate the relationship between streams and databases. And on Processing Streams on page 464 we will explore approaches and tools for processing those streams continually and ways  that they can be used to build applications.

## Transmitting Event Streams

- in the batch processing world, the inputs and outputs of a job are files (perhaps on a distributed filesystem). What does streaming look like?
- Files are typically parsed as a sequence of records. In stream processing, a record is basically an event

> event: a small, selfcontained, immutable object containing the details of something that happened at some point in time.

- events usually have timstamps.
- The thing that happened might be an action that a user took such as viewing a page or making a purchase. It might also originate from a machine such as a period measurement of a temperature sensor or a CPU utilization metric. In the example of Batch Processing with Unix Tools on pg 391, each line of the web server log is an event.
- Events may be encoded as a text string or JSON or binary as discussed in chapter 4 (they might have discussed thrift).
- This helps store an event like appending to a file or inserting into a table or writing to a document db. It also allows you to send the event over the network to another node in order ot process it.
- In batch processing, a file is written once and then potentially read by multiple jobs (similar to MapReduce)
- Analagously, in streaming technology an event is generated once by a producer also known as a publisher or sender. And then potentially processed by multiple consumers (subscribers or recipients). In a filesystem, a filename identifies a set of related records; in a streaming system, related events are usually grouped together into a topic or stream.
- In principle a file or db is sufficient to connect producers and consumers:

> Producer: writes every event that it generates to the datastore.
> Consumers: periodically polls the datastore to check for events that have appeared since it last ran.

- This is what a batch processor does at the end of each day.
- **However, when moving toward continual processging with low delays, polling becomes expensive if the datastore is not designed for this kind of usage. The more often you poll, the lower the percentage of requests that return new events, and thus the higher the overheads become. Instead it is better for consumers to be notified when new events appear.**
- RDBs have triggers which can react to a change but they are very limited in what they can do and have been somewhat of an afterthought in the database design. Instead specialized  tools have been developed for the purpose fo delivering event notifcations.

## Messaging Systems
- A common approach for notifying consumers about new events is to use a messaging system, a producer sends a message containing the event. which is then pushed to consumers.
- A direct communication channel like a UNIX pipe or TCP connections between prod and cons would be a simple way however most messaging systems expand on this basic model.
- UNIX pipes and TCP connect exactly one sender with one recipient too whereas a messaging system allows multiple prod nodes and multiple cons nodes to send and recieve messages via a topic.
- Within this pub/sub model, different systems take a wide range of approaches and there is no one right answer for all purposes!
- To differentiate the systems it is particularly helpful to ask 2 questions:

1. What happens if producers send messages faster than the consumers can process them?
- System can drop messages
- buff messages in a queue
- apply backpressure (flow control), blocking producer from sending messages. (seen in the Unix pipe or TCP when there is a buffer that if full can't accept messages, until the buffer is freed up in capacity).

2. What happens if nodes crash or temporarily go offline?
- Data loss?
- durability may require some combination of writing to disk and/or replication which has a cost
- If you can afford to sometimes lose messages you can probably get higher throughput and lower latency on the same hardware.

- Whether message loss is acceptable depends very much on the application. sensor readings and metrics that are transmitted periodically can have an occasional missing data point because in the grand scheme of things this probably doesn't matter that much.
- However, beware that if a large number of messages are dropped



